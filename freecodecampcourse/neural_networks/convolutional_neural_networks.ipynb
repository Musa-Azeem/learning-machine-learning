{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Network looks at entire image (global scale)\n",
    "- it looks at patterns in the entire image - image must be centered, etc.\n",
    "- it cannot recognize local patterns if they were moved to another part of the image\n",
    "\n",
    "#### Convolutional Neural Network looks at parts of image (local scale)\n",
    "- can learn local patterns and find them anywhere in image\n",
    "- CNN scans image to find features and passes those features to a dense classifier\n",
    "\n",
    "#### CNN Architecture:\n",
    "- Not densly connected\n",
    "- multiple layers used to pick up on complex patterns\n",
    "    - first layer may pick on edges and lines\n",
    "    - second layer takes this as input and may start forming shapes\n",
    "    - last layer might look at shapes and determine if they form a pattern\n",
    "\n",
    "#### Features Maps:\n",
    "- A 3D tensor with two spacial axes (width and height) and one depth axis\n",
    "- CNN layers take feature maps as input and return a new feature map that\n",
    "represent the presence of specific filters from the previous feature map\n",
    "- this is called a response map\n",
    "\n",
    "#### Layer Parameters - CNN defined by two key parameters\n",
    "- **Filter**: *m* x *n* pattern of pixels that we are looking for in image\n",
    "    - number of filters in CNN represents how many patterns each layer is\n",
    "    looking for and what the depth of our response map will be\n",
    "    - each layer of depth in the reponse map is a matrix containing values\n",
    "    inicating if each filter was present at that location or not (find by calculating dot product of sample and filter)\n",
    "    - trainable parameter\n",
    "- **Sample Size**: each layer is going to examine *n* x *m* blocks of pixels in each image\n",
    "    - typically, 3x3, or 5x5 blocks (sample size)\n",
    "    - sampling size is same size as filter\n",
    "    - layers work by sliding filers of *n* x *m* pixels over every possible position in our image\n",
    "    and populating a new response map indicating whether or not the filter is present at each location\n",
    "\n",
    "#### Pooling\n",
    "- Simplify process by reducing size of feature maps\n",
    "- takes average, max, or min value in a 2x2 area of feature map, and make that whole area into one pixel in new map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "Three Dimensions:\n",
    "- Image Height\n",
    "- Image Width\n",
    "- Color Channels\n",
    "\n",
    "Color Channels:\n",
    "- Image is made of several layers, one for the values of each color\n",
    "- for rgb, red, green, and blue each have their own layers, with pixel values from 0-255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Problem: Classify 10 different everyday objects using the CIFAR Image dataset in tensorflow\n",
    "\n",
    "It contains 60,000 32x32 color images with 6000 images of each class\n",
    "\n",
    "It has the following labels:\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 23:22:54.145399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-13 23:22:54.291992: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-13 23:22:54.292006: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-13 23:22:54.825462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-13 23:22:54.825509: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-13 23:22:54.825515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load and split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### CNN Architecture\n",
    "Common architecture is a stack of Conv2D and MaxPooling2D layers followed by a few dense layers\n",
    "- stack of convolutional and maxPooling layers extract the features from the image\n",
    "    - maxPooling layer after each convolutional layer reducing map size with max pixel value\n",
    "- features are flattened and fed to densly connected layers that determine the class of an image based on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# ---------------- Convolutional Base ----------------\n",
    "\n",
    "# Layer 1: input shape of data is 32x32x3 - will process 32 filters of size 3x3 over input data - will use relu activation function\n",
    "#          output map of this layer will be 30x30x32 - 30x30 instead of 32x32 bc no padding - last dimen bc 32 filters\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))\n",
    "\n",
    "\n",
    "# Layer 2: Preform Max Pooling operation using 2x2 samples and a stride of 2 (shrink feature map by factor of 2 )\n",
    "#          output map will be 15x15x32  - reduce each layer of depth by factor of 2\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# Layer 3: Same as Layer 1, but the input feature map is the output of layer 1 (after max poolng)\n",
    "#          Also increases frequency of filters from 32 to 64 (can afford this since the feature map size is shrinking from pooling)\n",
    "#          Output map will be 13x13x64 - lose two pixels bc no padding - 64 filters\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "# Layer 4: Same as layer 2\n",
    "#          Output shape will be 6x6x64 - reduce by factor of 2\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# Layer 5: Same as layer 3\n",
    "#          Output shape will be 4x4x64  - same as 1 and 3\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "# ------------------- Dense Layers -------------------\n",
    "\n",
    "# Layer 6: Flatten the matrices of feature maps to one dimension - Output shape is 1x1024\n",
    "model.add(layers.Flatten()) \n",
    "\n",
    "# Layer 7: 64 neuron dense layer to predict based on identified features - output later is 1x64 (one output for each neuron)\n",
    "model.add(layers.Dense(64))\n",
    "\n",
    "# Layer 8: 10 neuron output layer for 10 classes - output shape is 1x10 (probability distribution of each class)\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train Model\n",
    "Define the loss function, the optimizer, the metrics to track, and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',       # Choose the adam algorithm to preform gradient descent\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Function to claculate the loss\n",
    "    metrics=['accuracy']    # Keep track of accuracy during training\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_images,           # Train Images\n",
    "    train_labels,           # Train labels\n",
    "    epochs=8,              # Choose 10 epochs\n",
    "    validation_data=(test_images, test_labels)  # Testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Entries\n",
    "predictions = model.predict(test_images)    # probability distribution of each class for each image\n",
    "\n",
    "entry = 1   # entry of images to look at\n",
    "\n",
    "# Print expected label\n",
    "print(f\"Expected: {class_names[test_labels[entry][0]]}\")\n",
    "\n",
    "# show image of entry\n",
    "plt.figure()\n",
    "plt.imshow(test_images[entry])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# get the maximum value in the list to get the predicted class - argmax returns index of largest value\n",
    "print(f\"Predicted: {class_names[np.argmax(predictions[entry])]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Small Datasets\n",
    "It is difficult to train a CNN from scratch in situations where you don't have millions of images\n",
    "\n",
    "## Data Augmentation\n",
    "Create a larger dataset from a smaller one - avoid overfitting\n",
    "\n",
    "Perfoms random transformations on images so that model can generalize better\n",
    "- transformations like compressions, rotations, stretches, color changes\n",
    "\n",
    "Can be done with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "import keras.utils as image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Creates a data generator object that transforms images\n",
    "datagen = ImageDataGenerator (\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "\n",
    "# pick an image to transform\n",
    "test_img = train_images[20]\n",
    "img = image.img_to_array(test_img)\n",
    "img = img.reshape((1,) + img.shape) # shape of 1 by (figure it out) + image shape\n",
    "\n",
    "# Generate and save images\n",
    "for i, batch in enumerate(datagen.flow(img, save_prefix='test', save_format='jpeg')):\n",
    "    plt.figure(1)\n",
    "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models\n",
    "Can encorporate pretrained model in our own model - fine tune last few layers\n",
    "- base layers are able to pick up features very well that are common to all images\n",
    "- we can add layers to fine tune model for our problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Use cats vs dogs dataset from tensorflow\n",
    "Dataset contains (image, label) pairs where images have different dimensions and 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Split dataset into 80% training, 10% testing, and 10% validation\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Need to make all images the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160  # all images will be sizes to 160x160\n",
    "\n",
    "def format_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)  # cast every pixel to float\n",
    "    image = (image/127.5) - 1           # half of 255\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))    # resize image\n",
    "    return image, label\n",
    "\n",
    "# Apply to images\n",
    "train = raw_train.map(format_image)\n",
    "validation = raw_validation.map(format_image)\n",
    "test = raw_test.map(format_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a Pretrained Model\n",
    "Using MobileNet V2\n",
    "- trained on 1.4 million images and has 1000 different classes\n",
    "\n",
    "Only want to include the convolutional base of this model\n",
    "\n",
    "Will load the architecture and the weights of this model\n",
    "\n",
    "- Input is our 1x160x160x3 image\n",
    "- Final output layer of base CNN will be 32x5x5x1280, with a RelU activation function\n",
    "    - 5x5 is the size of each filter\n",
    "    - 32 is the 32 layers of different filters/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the Base Model from pretrained MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,      # Give the input shape for this application\n",
    "    include_top=False,          # Don't include the top layers that classify to the 1000 classes\n",
    "    weights='imagenet'          # Use the model weights from imagenet Google dataset\n",
    ")\n",
    "\n",
    "# Freeze the base - don't want to change the parameters of the base when training the classifier\n",
    "base_model.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Classfier\n",
    "- Instead of flattening, use global average pooling layer that will average the entire 5x5 area of each 2D feature map and return a single 1280 element vector per filter\n",
    "- add a single dense neuron as prediction layer and output - only 2 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "# Combine prediction layer with base\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,             # CNN base\n",
    "    global_average_layer,   # Flattening layer\n",
    "    prediction_layer        # Output Layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Train just the top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001     # how much we are allowed to modify the network while training - set low\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),   # optimizer\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),   # Only two classes, so use binary\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=3,\n",
    "    validation_data=validation_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"dogs_vs_cats.h5\")\n",
    "\n",
    "# Load model\n",
    "new_model = tf.keras.models.load_model('dogs_vs_cats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = model.predict(test_batches, batch_size=BATCH_SIZE, verbose=2)    # probability distribution of each class for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entry = 0   # entry of images to look at\n",
    "\n",
    "# Print expected label\n",
    "image, label = list(raw_test.take(entry+1))[-1]\n",
    "expected = metadata.features['label'].int2str(label)\n",
    "print(f\"Expected: {expected}\")\n",
    "\n",
    "\n",
    "# show image of entry\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# get the maximum value in the list to get the predicted class - argmax returns index of largest value\n",
    "prediction = 0 if predictions[entry] <= .5 else 1\n",
    "print(f\"Predicted: {metadata.features['label'].int2str(prediction)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
