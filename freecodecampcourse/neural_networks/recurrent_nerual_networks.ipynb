{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "More capable in processing sequential data like text\n",
    "\n",
    "Commonly used for **natural language processing**\n",
    "\n",
    "### Internal Loop\n",
    "- Recurrent Neural Networks does not process entire data at once - processes at different time steps\n",
    "    - for text, feed one word at a time\n",
    "- Model maintains an internal memory - remembers what it has seen previously\n",
    "- Types of RNN layers:\n",
    "\n",
    "#### Simple RNN Layer\n",
    "- Data is passed as sequence\n",
    "- There is a recurrent layer in the network, that has a loop back to itself\n",
    "- The recurrent layer for input at time 1 has an input from the layer of the input at time 0, from the previous step\n",
    "    - At time step 0, the only input is the input data, x<sub>0</sub>, and it produces an output h<sub>0</sub>\n",
    "    - At time step 1, the input to the recurrent layer is x<sub>1</sub> as well as h<sub>0</sub>, which produces an output h<sub>1</sub>\n",
    "    - this repeats for the next time step \n",
    "- Each time step builds on everything seen before\n",
    "- issue: for long sequences, the impact of the older timestep inputs can be lost, because only the most recent timestep is fed back\n",
    "\n",
    "#### LSTM layer\n",
    "- Long-Short Term Memory\n",
    "- Allows the model to remember the output state at any time in the past\n",
    "\n",
    "## Data\n",
    "\n",
    "### Sequence Data\n",
    "- Long chains of text, weather patterns, videos, or anything where the notion of a step of time is relevent\n",
    "- The order of the data is important to keep track of\n",
    "\n",
    "### Textual Data\n",
    "- A type of sequence data\n",
    "- need to encode the text into numrical data that can be fed to the neural network\n",
    "- There are different methods of doing this:\n",
    "\n",
    "    #### Bag of Words\n",
    "    - Look at entire training dataset and create a dictionary of the vocabalary\n",
    "        - every unique word is the vocabulary\n",
    "        - some integer represents each word\n",
    "    - keep track of the frequency of each word in a sentence\n",
    "    - flawed method because the order of words is lost - only keeps frequency and what words they are\n",
    "    \n",
    "    #### Word Embedding\n",
    "    - Tries to represent similar words with similar numbers\n",
    "    - classify each word in n dimensional vectors (usually 64 or 128)\n",
    "        - vector tells how similar word is to other words\n",
    "        - the words \"good\" and \"happy\" will be represent by vectors with a small angle between them\n",
    "        - opposite words will have very different vectors\n",
    "    - word embedding is implemented in a layer in the neural netword\n",
    "        - model learns word embeddings through the context of the words in the sentence\n",
    "    - can use pretrained word embedding layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Analyze how positive or negative a piece of text is"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset\n",
    "- IMDB movie review dataset from keras\n",
    "- contains 25,000 movie reviews\n",
    "- reviews are preprocessed and have labels as either positive or negative\n",
    "    - each review is encoded by integers that represent how common the word is in the entire dataset\n",
    "    - a word encoded by integer 3 is the 3rd most common word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m sequence\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "VOCAB_SIZE = 88584      # Number of unique words in this dataset\n",
    "\n",
    "MAX_LEN = 250           # Max word length of review we will use\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE) # include all of the words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Need to make all samples the same length of words\n",
    "- if review is greater than 250 words, trim off extra words\n",
    "- if review is less than 250 words, add 0s to make it 250 (padding to the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAX_LEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "- First layer is the embedding layer to find a meaningful representation of numbers\n",
    "- Second layer is the LSTM feedback layer\n",
    "- Third layer is a Dense classification layer - sigmoid activation to get probabilty of positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),      # Embedding layer - words are going to represented as 32 dimen vectors\n",
    "    tf.keras.layers.LSTM(32),                       # LSTM feedback layer - input is 32 dimensions per word\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # One output neuron\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels, \n",
    "    epochs=10, \n",
    "    validation_split=0.2    # Validate with 20% of data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(f\"Accuracy: {results[1]*100:0.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction\n",
    "- Need to preprocess any reviews in same method that original data was encoded in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAX_LEN)[0] # returns list of lists, get first one\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,MAX_LEN))    # shape of input is 1 review with MAX_LEN (250) words\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)[0][0]\n",
    "    if result >= 0.5:\n",
    "        print(f\"Predicted: Positive\")\n",
    "    else:\n",
    "        print(f\"Predicted: Negative\")\n",
    "\n",
    "review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(review)\n",
    "\n",
    "review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(review)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Generation\n",
    "Generate the next characters in a sequence of text\n",
    "\n",
    "### RNN Play Generator\n",
    "- Show the neural network an example o something for it create until it learn to write it iteself\n",
    "- Use character predictive model that will take a variable length input sequence and predict the next character\n",
    "- Using the model many times in a row with the previous output from the last prediction can generate a sequence\n",
    "\n",
    "### Dataset\n",
    "- Romeo and Juliet dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Load dataset\n",
    "response = requests.get(\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "text = response.content.decode(encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Encode each character in text with an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))   # get unique characters\n",
    "\n",
    "# Create encode mapping\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convert text to integer encoding\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Examples\n",
    "\n",
    "Need to split text into shorter sequences to pass to model as training examples\n",
    "\n",
    "Input will be an *n* length sequence and output will be an *n* length sequence which is the input shifted once letter to the right\n",
    "- EX: input: Hell -> output: ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100   # length of each training example\n",
    "examples_per_epoch = len(text) // (seq_len)\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256     # Dimensions of embedded encoding of word vectors\n",
    "RNN_UNITS = 1024        # \n",
    "BUFFER_SIZE = 10000     # Buffer to use during shuffling\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # create character dataset from text as integer\n",
    "\n",
    "# Batch character dataset into 101 size batches, drop the extra text at the end\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]     # Get all but last character (chars 0-100)\n",
    "    target_text = chunk[1:]     # Get all but first character (chars 1-101) - the value to predict\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)     # split each entry in dataset\n",
    "\n",
    "# Make Batches for final training sequence\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "Use an embedding layer, and LSTM layer, and a dense layer that contains a node for each unique character it can choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            batch_input_shape=[batch_size, None]        # size BATCH_SIZE x None : None -> don't know length of input sequence when making predictions later\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units, \n",
    "            return_sequences=True,      # Return the intermediate stage at every step - want to see intermediate steps, not just final stage\n",
    "            stateful=True,\n",
    "            recurrent_initializer='glorot_uniform'  # starting values in LSTM\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Loss Function\n",
    "Model will output a (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character\n",
    "- batches are 64, for training, sequence length is 100, and there are 65 vocabulary words\n",
    "- There are 64 ouput array - 1 for each sequence in the batch\n",
    "- There are 100 arrays inside each of the 64 - one prediction for each character in the array (remembering all of them)\n",
    "- There are 65 values in each of these - the probability distribution for each character being the prediction\n",
    "\n",
    "Since the output shape is not common, need to make our own loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65)\n",
      "(100, 65)\n",
      "tf.Tensor(\n",
      "[[ 6.1521893e-03  3.7606128e-03 -8.3994935e-04 ... -2.2678513e-03\n",
      "   2.8230438e-03  5.0888341e-03]\n",
      " [ 5.8572795e-03  5.2563050e-03  5.6081782e-03 ... -2.4424936e-03\n",
      "  -7.1421098e-03  6.9877715e-05]\n",
      " [ 2.3559784e-03  2.4909736e-03  8.3274571e-03 ...  3.3260849e-03\n",
      "  -2.5877107e-03  4.3944325e-03]\n",
      " ...\n",
      " [ 5.5909844e-04 -4.5386464e-03 -1.1454150e-04 ... -1.4851980e-03\n",
      "   2.2543040e-03 -2.2659467e-03]\n",
      " [ 2.9147640e-03 -2.4964965e-03 -5.4207887e-04 ... -1.6330150e-03\n",
      "   1.8414885e-03  3.9077187e-03]\n",
      " [ 8.1501398e-03  3.9647724e-03  1.3153135e-03 ... -1.2689475e-03\n",
      "   4.3814261e-03  4.4938019e-03]], shape=(100, 65), dtype=float32)\n",
      "(65,)\n",
      "tf.Tensor(\n",
      "[ 0.00615219  0.00376061 -0.00083995  0.00180282  0.00013687  0.0022212\n",
      " -0.00454704 -0.00905489 -0.00017375  0.00349678 -0.00064473 -0.00045719\n",
      "  0.00163089 -0.00304316 -0.00170851  0.00442004 -0.00225726  0.001726\n",
      "  0.00239011  0.00727295 -0.00191833 -0.0019024   0.00155215 -0.0014876\n",
      " -0.00635435  0.00471327 -0.00567613 -0.00034869  0.00303439  0.00214756\n",
      "  0.00703284 -0.00391473  0.00109474 -0.00489688  0.00248763 -0.00182034\n",
      " -0.00132622 -0.00099382  0.0012284  -0.00011262 -0.00592375  0.00106497\n",
      " -0.00404334  0.0016685  -0.0041085  -0.0031594   0.00122632 -0.00281223\n",
      " -0.00259296  0.00594296  0.00138179 -0.00304456  0.00437808 -0.00995361\n",
      " -0.00295493  0.00568617  0.00272128 -0.00258115 -0.000958    0.00648409\n",
      " -0.00015553 -0.00293939 -0.00226785  0.00282304  0.00508883], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# See ouput shape by predicting first batch (model not trained, so prediction is random)\n",
    "for input, target in data.take(1):\n",
    "    predictions = model(input)\n",
    "\n",
    "print(predictions.shape)\n",
    "# print(predictions)  # array of 64 arrays - one for each sequence in the batch\n",
    "\n",
    "pred = predictions[0] # array of 100 probability distributions for the first sequence in the batch\n",
    "print(pred.shape)\n",
    "print(pred)\n",
    "\n",
    "time_pred = pred[0]  # probabillity distribution for the prediction for the first character in the sequence (first timestep)\n",
    "print(time_pred.shape)\n",
    "print(time_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".CJtH'leWh!HB BNl\\ngUZQtDp3TC&VlljYQQ&.&ORuhigyoIuA-cnY:y-JUnfUJSL!lYPdzP-FNHdSzDfRSm;I-l:3NjNtvViGl3\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most confident prediction for each timestep:\n",
    "sampled_indeces = tf.random.categorical(pred, num_samples=1) # sample the output distribution (not the highest probability, that's not good for loss function)\n",
    "\n",
    "# reshape and convert to integers\n",
    "sampled_indeces = np.reshape(sampled_indeces, (1,-1))[0]\n",
    "predicted_chars = int_to_text(sampled_indeces)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits): # logits is probability distributions\n",
    "    # use built in function to find how different the predicted values were from the labels\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metric=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Checkpoints\n",
    "- allow model to save checkpoints as it trains\n",
    "- can go back and load model from checkpoint and continue training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'chpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session() # Clears any training to model (destroys current graph and creates new one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 257s 1s/step - loss: 2.6011\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=1, callbacks=[checkpoint_callback], )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "- Rebuild the model from checkpoint using a batch size of 1 to feed one pice of test to the model to test it\n",
    "- then, we can load weights from the latest checkpoint to retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))      # expect input of one and unknown second dimension (sequence length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)  # turns single list [1,2,3] into double list [[1,2,3]], as expected by model (single element batch)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)  # remove exterior dimension of prediction - turns [[1,2,3]] into [1,2,3]\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()  # \n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"romeour, a sealt an wnos, theel'-fost derprcer apminging yo, wet andthe,\\nWhest me wis not for sot devame, Bethitquaed rath.\\n\\nCARIUTI IUCISRE\\nID:\\nNave the praikit weat of int eanend.\\n\\nPatore,\\nAnd nate and veikntowand,\\n: mane not e emaligast bu ge a mes ore priathy thom, sull co murs, miks, what swacling fout you groukly minde youl lomelavi, fory thimy formy,\\nand of will som wilg hissing meopt,\\nIs nom bgenutndioks: my gaidessnted sond, my is itcengy, I withan; now hak' notheinth brilimdmy, by marderou; th thiallor wongs, whous taeringt\\nAf phende bede yours hus cryoling sind dost abusith, of not in woth that in thagh\\nI dazentive,\\nHome, swe greead, Magiss,\\nAnath not andereen'g tor bug thar suthts conest,\\nI'd if sorou wuy pepe,\\nEnd in you grond.\\n\\nDOMOGILUS:\\nSitt s and hor Ry\\nChan, ane fore cinty tho\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"romeo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2d3a483379b1c7673981d83053148f2582e1f4aa062f00e0ad0fc5c82e95812"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
