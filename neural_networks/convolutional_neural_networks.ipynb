{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Network looks at entire image (global scale)\n",
    "- it looks at patterns in the entire image - image must be centered, etc.\n",
    "- it cannot recognize local patterns if they were moved to another part of the image\n",
    "\n",
    "#### Convolutional Neural Network looks at parts of image (local scale)\n",
    "- can learn local patterns and find them anywhere in image\n",
    "- CNN scans image to find features and passes those features to a dense classifier\n",
    "\n",
    "#### CNN Architecture:\n",
    "- Not densly connected\n",
    "- multiple layers used to pick up on complex patterns\n",
    "    - first layer may pick on edges and lines\n",
    "    - second layer takes this as input and may start forming shapes\n",
    "    - last layer might look at shapes and determine if they form a pattern\n",
    "\n",
    "#### Features Maps:\n",
    "- A 3D tensor with two spacial axes (width and height) and one depth axis\n",
    "- CNN layers take feature maps as input and return a new feature map that\n",
    "represent the presence of specific filters from the previous feature map\n",
    "- this is called a response map\n",
    "\n",
    "#### Layer Parameters - CNN defined by two key parameters\n",
    "- **Filter**: *m* x *n* pattern of pixels that we are looking for in image\n",
    "    - number of filters in CNN represents how many patterns each layer is\n",
    "    looking for and what the depth of our response map will be\n",
    "    - each layer of depth in the reponse map is a matrix containing values\n",
    "    inicating if each filter was present at that location or not (find by calculating dot product of sample and filter)\n",
    "    - trainable parameter\n",
    "- **Sample Size**: each layer is going to examine *n* x *m* blocks of pixels in each image\n",
    "    - typically, 3x3, or 5x5 blocks (sample size)\n",
    "    - sampling size is same size as filter\n",
    "    - layers work by sliding filers of *n* x *m* pixels over every possible position in our image\n",
    "    and populating a new response map indicating whether or not the filter is present at each location\n",
    "\n",
    "#### Pooling\n",
    "- Simplify process by reducing size of feature maps\n",
    "- takes average, max, or min value in a 2x2 area of feature map, and make that whole area into one pixel in new map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "Three Dimensions:\n",
    "- Image Height\n",
    "- Image Width\n",
    "- Color Channels\n",
    "\n",
    "Color Channels:\n",
    "- Image is made of several layers, one for the values of each color\n",
    "- for rgb, red, green, and blue each have their own layers, with pixel values from 0-255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Problem: Classify 10 different everyday objects using the CIFAR Image dataset in tensorflow\n",
    "\n",
    "It contains 60,000 32x32 color images with 6000 images of each class\n",
    "\n",
    "It has the following labels:\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load and split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### CNN Architecture\n",
    "Common architecture is a stack of Conv2D and MaxPooling2D layers followed by a few dense layers\n",
    "- stack of convolutional and maxPooling layers extract the features from the image\n",
    "    - maxPooling layer after each convolutional layer reducing map size with max pixel value\n",
    "- features are flattened and fed to densly connected layers that determine the class of an image based on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# ---------------- Convolutional Base ----------------\n",
    "\n",
    "# Layer 1: input shape of data is 32x32x3 - will process 32 filters of size 3x3 over input data - will use relu activation function\n",
    "#          output map of this layer will be 30x30x32 - 30x30 instead of 32x32 bc no padding - last dimen bc 32 filters\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))\n",
    "\n",
    "\n",
    "# Layer 2: Preform Max Pooling operation using 2x2 samples and a stride of 2 (shrink feature map by factor of 2 )\n",
    "#          output map will be 15x15x32  - reduce each layer of depth by factor of 2\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# Layer 3: Same as Layer 1, but the input feature map is the output of layer 1 (after max poolng)\n",
    "#          Also increases frequency of filters from 32 to 64 (can afford this since the feature map size is shrinking from pooling)\n",
    "#          Output map will be 13x13x64 - lose two pixels bc no padding - 64 filters\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "# Layer 4: Same as layer 2\n",
    "#          Output shape will be 6x6x64 - reduce by factor of 2\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# Layer 5: Same as layer 3\n",
    "#          Output shape will be 4x4x64  - same as 1 and 3\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "# ------------------- Dense Layers -------------------\n",
    "\n",
    "# Layer 6: Flatten the matrices of feature maps to one dimension - Output shape is 1x1024\n",
    "model.add(layers.Flatten()) \n",
    "\n",
    "# Layer 7: 64 neuron dense layer to predict based on identified features - output later is 1x64 (one output for each neuron)\n",
    "model.add(layers.Dense(64))\n",
    "\n",
    "# Layer 8: 10 neuron output layer for 10 classes - output shape is 1x10 (probability distribution of each class)\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train Model\n",
    "Define the loss function, the optimizer, the metrics to track, and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 20:30:35.445565: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561/1563 [============================>.] - ETA: 0s - loss: 1.4325 - accuracy: 0.4816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 20:30:52.393170: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 122880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4324 - accuracy: 0.4817 - val_loss: 1.1635 - val_accuracy: 0.5911\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0720 - accuracy: 0.6240 - val_loss: 1.0252 - val_accuracy: 0.6422\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9408 - accuracy: 0.6716 - val_loss: 0.9502 - val_accuracy: 0.6705\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8506 - accuracy: 0.7032 - val_loss: 0.9336 - val_accuracy: 0.6800\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7873 - accuracy: 0.7258 - val_loss: 0.9089 - val_accuracy: 0.6827\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.7361 - accuracy: 0.7418 - val_loss: 0.9518 - val_accuracy: 0.6769\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6865 - accuracy: 0.7613 - val_loss: 0.8793 - val_accuracy: 0.7028\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.6469 - accuracy: 0.7728 - val_loss: 0.8501 - val_accuracy: 0.7151\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.6117 - accuracy: 0.7868 - val_loss: 0.9337 - val_accuracy: 0.6979\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.5776 - accuracy: 0.7963 - val_loss: 0.9396 - val_accuracy: 0.7026\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',       # Choose the adam algorithm to preform gradient descent\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Function to claculate the loss\n",
    "    metrics=['accuracy']    # Keep track of accuracy during training\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_images,           # Train Images\n",
    "    train_labels,           # Train labels\n",
    "    epochs=8,              # Choose 10 epochs\n",
    "    validation_data=(test_images, test_labels)  # Testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
