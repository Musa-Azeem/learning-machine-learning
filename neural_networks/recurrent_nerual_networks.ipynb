{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "More capable in processing sequential data like text\n",
    "\n",
    "Commonly used for **natural language processing**\n",
    "\n",
    "### Internal Loop\n",
    "- Recurrent Neural Networks does not process entire data at once - processes at different time steps\n",
    "    - for text, feed one word at a time\n",
    "- Model maintains an internal memory - remembers what it has seen previously\n",
    "- Types of RNN layers:\n",
    "\n",
    "#### Simple RNN Layer\n",
    "- Data is passed as sequence\n",
    "- There is a recurrent layer in the network, that has a loop back to itself\n",
    "- The recurrent layer for input at time 1 has an input from the layer of the input at time 0, from the previous step\n",
    "    - At time step 0, the only input is the input data, x<sub>0</sub>, and it produces an output h<sub>0</sub>\n",
    "    - At time step 1, the input to the recurrent layer is x<sub>1</sub> as well as h<sub>0</sub>, which produces an output h<sub>1</sub>\n",
    "    - this repeats for the next time step \n",
    "- Each time step builds on everything seen before\n",
    "- issue: for long sequences, the impact of the older timestep inputs can be lost, because only the most recent timestep is fed back\n",
    "\n",
    "#### LSTM layer\n",
    "- Long-Short Term Memory\n",
    "- Allows the model to remember the output state at any time in the past\n",
    "\n",
    "## Data\n",
    "\n",
    "### Sequence Data\n",
    "- Long chains of text, weather patterns, videos, or anything where the notion of a step of time is relevent\n",
    "- The order of the data is important to keep track of\n",
    "\n",
    "### Textual Data\n",
    "- A type of sequence data\n",
    "- need to encode the text into numrical data that can be fed to the neural network\n",
    "- There are different methods of doing this:\n",
    "\n",
    "    #### Bag of Words\n",
    "    - Look at entire training dataset and create a dictionary of the vocabalary\n",
    "        - every unique word is the vocabulary\n",
    "        - some integer represents each word\n",
    "    - keep track of the frequency of each word in a sentence\n",
    "    - flawed method because the order of words is lost - only keeps frequency and what words they are\n",
    "    \n",
    "    #### Word Embedding\n",
    "    - Tries to represent similar words with similar numbers\n",
    "    - classify each word in n dimensional vectors (usually 64 or 128)\n",
    "        - vector tells how similar word is to other words\n",
    "        - the words \"good\" and \"happy\" will be represent by vectors with a small angle between them\n",
    "        - opposite words will have very different vectors\n",
    "    - word embedding is implemented in a layer in the neural netword\n",
    "        - model learns word embeddings through the context of the words in the sentence\n",
    "    - can use pretrained word embedding layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Analyze how positive or negative a piece of text is"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset\n",
    "- IMDB movie review dataset from keras\n",
    "- contains 25,000 movie reviews\n",
    "- reviews are preprocessed and have labels as either positive or negative\n",
    "    - each review is encoded by integers that represent how common the word is in the entire dataset\n",
    "    - a word encoded by integer 3 is the 3rd most common word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "VOCAB_SIZE = 88584      # Number of unique words in this dataset\n",
    "\n",
    "MAX_LEN = 250           # Max word length of review we will use\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE) # include all of the words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Need to make all samples the same length of words\n",
    "- if review is greater than 250 words, trim off extra words\n",
    "- if review is less than 250 words, add 0s to make it 250 (padding to the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAX_LEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "- First layer is the embedding layer to find a meaningful representation of numbers\n",
    "- Second layer is the LSTM feedback layer\n",
    "- Third layer is a Dense classification layer - sigmoid activation to get probabilty of positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),      # Embedding layer - words are going to represented as 32 dimen vectors\n",
    "    tf.keras.layers.LSTM(32),                       # LSTM feedback layer - input is 32 dimensions per word\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # One output neuron\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels, \n",
    "    epochs=10, \n",
    "    validation_split=0.2    # Validate with 20% of data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(f\"Accuracy: {results[1]*100:0.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction\n",
    "- Need to preprocess any reviews in same method that original data was encoded in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAX_LEN)[0] # returns list of lists, get first one\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,MAX_LEN))    # shape of input is 1 review with MAX_LEN (250) words\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)[0][0]\n",
    "    if result >= 0.5:\n",
    "        print(f\"Predicted: Positive\")\n",
    "    else:\n",
    "        print(f\"Predicted: Negative\")\n",
    "\n",
    "review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(review)\n",
    "\n",
    "review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(review)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Generation\n",
    "Generate the next characters in a sequence of text\n",
    "\n",
    "### RNN Play Generator\n",
    "- Show the neural network an example o something for it create until it learn to write it iteself\n",
    "- Use character predictive model that will take a variable length input sequence and predict the next character\n",
    "- Using the model many times in a row with the previous output from the last prediction can generate a sequence\n",
    "\n",
    "### Dataset\n",
    "- Romeo and Juliet dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Load dataset\n",
    "response = requests.get(\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "text = response.content.decode(encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Encode each character in text with an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))   # get unique characters\n",
    "\n",
    "# Create encode mapping\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convert text to integer encoding\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Examples\n",
    "\n",
    "Need to split text into shorter sequences to pass to model as training examples\n",
    "\n",
    "Input will be an *n* length sequence and output will be an *n* length sequence which is the input shifted once letter to the right\n",
    "- EX: input: Hell -> output: ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100   # length of each training example\n",
    "examples_per_epoch = len(text) // (seq_len)\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256     # Dimensions of embedded encoding of word vectors\n",
    "RNN_UNITS = 1024        # \n",
    "BUFFER_SIZE = 10000     # Buffer to use during shuffling\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # create character dataset from text as integer\n",
    "\n",
    "# Batch character dataset into 101 size batches, drop the extra text at the end\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]     # Get all but last character (chars 0-100)\n",
    "    target_text = chunk[1:]     # Get all but first character (chars 1-101) - the value to predict\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)     # split each entry in dataset\n",
    "\n",
    "# Make Batches for final training sequence\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "Use an embedding layer, and LSTM layer, and a dense layer that contains a node for each unique character it can choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            batch_input_shape=[batch_size, None]        # size BATCH_SIZE x None : None -> don't know length of input sequence when making predictions later\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units, \n",
    "            return_sequences=True,      # Return the intermediate stage at every step - want to see intermediate steps, not just final stage\n",
    "            stateful=True,\n",
    "            recurrent_initializer='glorot_uniform'  # starting values in LSTM\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = built_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
